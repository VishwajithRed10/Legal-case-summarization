{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3348,"status":"ok","timestamp":1714733188874,"user":{"displayName":"Adhvik Murarisetty","userId":"18081365484680489972"},"user_tz":-330},"id":"4e7qzJN_Blqy","outputId":"4bf93083-f5f9-41ff-ba20-e8f87cc508e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16227,"status":"ok","timestamp":1714733205097,"user":{"displayName":"Adhvik Murarisetty","userId":"18081365484680489972"},"user_tz":-330},"id":"4zGMGKPHBonY","outputId":"6d44666c-74e1-4647-aad0-760749cc9d69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch_lightning==1.7.0 in /usr/local/lib/python3.10/dist-packages (1.7.0)\n","Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (8.1.7)\n","Requirement already satisfied: keybert in /usr/local/lib/python3.10/dist-packages (0.8.4)\n","Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (2.10.1)\n","Requirement already satisfied: torchmetrics==0.11.4 in /usr/local/lib/python3.10/dist-packages (0.11.4)\n","Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.25.1)\n","Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: summarizer in /usr/local/lib/python3.10/dist-packages (0.0.7)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (1.25.2)\n","Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (2.2.1+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (4.66.2)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (6.0.1)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (2023.6.0)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (2.15.2)\n","Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (0.3.2)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (24.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning==1.7.0) (4.11.0)\n","Requirement already satisfied: alembic in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.1)\n","Requirement already satisfied: cliff in /usr/local/lib/python3.10/dist-packages (from optuna) (4.6.0)\n","Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from optuna) (0.10.0)\n","Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n","Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.11.4)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.3)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.40.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.31.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n","Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.10/dist-packages (from keybert) (2.7.0)\n","Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from keybert) (1.2.2)\n","Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from keybert) (13.7.1)\n","Requirement already satisfied: rapidfuzz<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.9.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2023.12.25)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from summarizer) (3.8.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.0) (3.9.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.4.0->keybert) (2.16.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=0.3.8->keybert) (9.4.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.1.0->optuna) (3.0.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (1.63.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (3.6)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (67.7.2)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->pytorch_lightning==1.7.0) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (3.1.3)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.*->pytorch_lightning==1.7.0) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9.*->pytorch_lightning==1.7.0) (12.4.127)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.3)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic->optuna) (1.3.3)\n","Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from cliff->optuna) (3.10.0)\n","Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from cliff->optuna) (0.5.2)\n","Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from cliff->optuna) (2.4.3)\n","Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from cliff->optuna) (5.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.51.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.1.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.0) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.0) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.0) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.0) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.0) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning==1.7.0) (4.0.3)\n","Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.10/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n","Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.13)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.0) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.0) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.0) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->pytorch_lightning==1.7.0) (1.3.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n","Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from stevedore>=2.0.1->cliff->optuna) (6.0.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch_lightning==1.7.0) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.*->pytorch_lightning==1.7.0) (1.3.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning==1.7.0) (0.6.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->pytorch_lightning==1.7.0) (3.2.2)\n"]}],"source":["!pip install pytorch_lightning==1.7.0 bert_score click keybert optuna torchmetrics==0.11.4 optuna==2.10.1 Levenshtein sacrebleu sacremoses summarizer"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":561,"status":"ok","timestamp":1714733977389,"user":{"displayName":"Adhvik Murarisetty","userId":"18081365484680489972"},"user_tz":-330},"id":"cxwDA0NfHADS"},"outputs":[],"source":["!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":356,"status":"ok","timestamp":1714733953225,"user":{"displayName":"Adhvik Murarisetty","userId":"18081365484680489972"},"user_tz":-330},"id":"p7WJKXxCGk6P"},"outputs":[],"source":["import os\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"elapsed":1428,"status":"error","timestamp":1714734011104,"user":{"displayName":"Adhvik Murarisetty","userId":"18081365484680489972"},"user_tz":-330},"id":"YSLYWDMJBiQf","outputId":"02825219-d779-4914-c2f5-a117a016a582"},"outputs":[],"source":["import torch\n","#torch.multiprocessing.set_start_method('forkserver', force=True)\n","from pathlib import Path\n","\n","from drive.MyDrive.Group_24.preprocessor import WIKI_DOC, D_WIKI, EXP_DIR, MILDSUM\n","import time\n","import json\n","\n","#from contextlib import contextmanager\n","import argparse\n","\n","from argparse import ArgumentParser\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from optuna.integration import PyTorchLightningPruningCallback\n","\n","# from T5_2 import SumSim, train\n","from drive.MyDrive.Group_24.Bart2 import SumSim, train\n","# from Bart_baseline_finetuned import BartBaseLineFineTuned, train\n","# from T5_baseline_finetuned import T5BaseLineFineTuned, train\n","\n","torch.cuda.empty_cache()\n","def parse_arguments():\n","    p = ArgumentParser()\n","\n","    p.add_argument('--seed', type=int, default=42, help='randomization seed')\n","\n","    p = SumSim.add_model_specific_args(p)\n","    # p = BartBaseLineFineTuned.add_model_specific_args(p)\n","    # p = T5BaseLineFineTuned.add_model_specific_args(p)\n","    p = pl.Trainer.add_argparse_args(p)\n","    args,_ = p.parse_known_args()\n","    return args\n","\n","\n","\n","# Create experiment directory\n","def get_experiment_dir(create_dir=False):\n","    dir_name = f'{int(time.time() * 1000000)}'\n","    path = EXP_DIR / f'exp_{dir_name}'\n","    if create_dir == True: path.mkdir(parents=True, exist_ok=True)\n","    return path\n","\n","def log_params(filepath, kwargs):\n","    filepath = Path(filepath)\n","    kwargs_str = dict()\n","    for key in kwargs:\n","        kwargs_str[key] = str(kwargs[key])\n","    json.dump(kwargs_str, filepath.open('w'), indent=4)\n","\n","\n","\n","def run_training(args, dataset):\n","\n","    args.output_dir = get_experiment_dir(create_dir=True)\n","    # logging the args\n","    log_params(args.output_dir / \"params.json\", vars(args))\n","\n","    args.dataset = dataset\n","    print(\"Dataset: \",args.dataset)\n","    train(args)\n","\n","\n","\n","dataset = MILDSUM\n","args = parse_arguments()\n","run_training(args, dataset)\n","torch.cuda.empty_cache()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1714733247721,"user":{"displayName":"Adhvik Murarisetty","userId":"18081365484680489972"},"user_tz":-330},"id":"NktNdrOABiQg"},"outputs":[],"source":["import click\n","from drive.MyDrive.Group_24.easse.utils.resources import get_orig_sents, get_refs_sents\n","\n","def get_sys_sents(test_set, sys_sents_path=None):\n","    # Get system sentences to be evaluated\n","    if sys_sents_path is not None:\n","        return read_lines(sys_sents_path)\n","    else:\n","        # read the system output\n","        with click.get_text_stream(\"stdin\", encoding=\"utf-8\") as system_output_file:\n","            return system_output_file.read().splitlines()\n","\n","\n","def get_orig_and_refs_sents(test_set, orig_sents_path=None, refs_sents_paths=None):\n","    # Get original and reference sentences\n","    if test_set == \"custom\":\n","        assert orig_sents_path is not None\n","        assert refs_sents_paths is not None\n","\n","\n","        orig_sents = read_lines(orig_sents_path)\n","        # NOTE: refs_sents_paths is a list of paths\n","        refs_sents = [read_lines(refs_sents_paths)]\n","        # if type(refs_sents_paths) == str:\n","        #     refs_sents_paths = refs_sents_paths.split(\",\")\n","        #refs_sents = [read_lines(ref_sents_path) for ref_sents_path in refs_sents_paths]\n","    else:\n","        orig_sents = get_orig_sents(test_set)\n","        refs_sents = get_refs_sents(test_set)\n","    \n","    return orig_sents, refs_sents"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1714733247721,"user":{"displayName":"Adhvik Murarisetty","userId":"18081365484680489972"},"user_tz":-330},"id":"0XqIcIA0BiQh"},"outputs":[],"source":["from pathlib import Path\n","import sys\n","import os\n","import shutil\n","\n","from drive.MyDrive.Group_24.easse.cli import evaluate_system_output\n","#from Ts_T5 import T5FineTuner\n","from drive.MyDrive.Group_24.easse.report import get_all_scores\n","from contextlib import contextmanager\n","import json\n","#from preprocessor import Preprocessor\n","import torch\n","#from transformers import T5ForConditionalGeneration, T5TokenizerFast\n","from drive.MyDrive.Group_24.preprocessor import get_data_filepath, EXP_DIR,  REPO_DIR, WIKI_DOC, D_WIKI\n","from drive.MyDrive.Group_24.preprocessor import write_lines, yield_lines, count_line, read_lines, generate_hash\n","from drive.MyDrive.Group_24.easse.sari import corpus_sari\n","import time\n","from drive.MyDrive.Group_24.utils.D_SARI import D_SARIsent, D_SARIfile\n","\n","@contextmanager\n","def log_stdout(filepath, mute_stdout=False):\n","    '''Context manager to write both to stdout and to a file'''\n","\n","    class MultipleStreamsWriter:\n","        def __init__(self, streams):\n","            self.streams = streams\n","\n","        def write(self, message):\n","            for stream in self.streams:\n","                stream.write(message)\n","\n","        def flush(self):\n","            for stream in self.streams:\n","                stream.flush()\n","\n","    save_stdout = sys.stdout\n","    log_file = open(filepath, 'w')\n","    if mute_stdout:\n","        sys.stdout = MultipleStreamsWriter([log_file])  # Write to file only\n","    else:\n","        sys.stdout = MultipleStreamsWriter([save_stdout, log_file])  # Write to both stdout and file\n","    try:\n","        yield\n","    finally:\n","        sys.stdout = save_stdout\n","        log_file.close()\n","\n","# set random seed universal\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","\n","set_seed(42)\n","model_dir = None\n","_model_dirname = None\n","max_len = 256\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using {device}')\n","\n","# specify the model_name and checkpoint_name\n","\n","# model_dirname = 'exp_WikiDocFinal_T5_kw_num4_div0.9_0.001CosSim(ReLU_W)'\n","# checkpoint_path = 'checkpoint-epoch=5.ckpt'\n","\n","#### Single model ####\n","# load the model\n","# Model = BartBaseLineFineTuned.load_from_checkpoint('/content/experiments/checkpoint-epoch=0.ckpt').to(device)\n","# model = Model.model.to(device)\n","# tokenizer = Model.tokenizer\n","#### Single model ####\n","\n","#### Joint model ####\n","Model = SumSim.load_from_checkpoint('/content/experiments/checkpoint-epoch=2.ckpt').to(device)\n","summarizer = Model.summarizer.to(device)\n","simplifier = Model.simplifier.to(device)\n","summarizer_tokenizer = Model.summarizer_tokenizer\n","simplifier_tokenizer = Model.simplifier_tokenizer\n","#### Joint model ####\n","\n","# def generate_single(sentence, preprocessor = None):\n","#     '''\n","#     This function is for T5 or Bart single model to generate/predict\n","#     '''\n","\n","#     # text = \"simplify: \" + sentence  ### -> for T5\n","#     text = sentence\n","#     encoding = tokenizer(text, max_length=512,\n","#                                      padding='max_length',\n","#                                      truncation=True,\n","#                                      return_tensors=\"pt\")\n","#     input_ids = encoding[\"input_ids\"].to(device)\n","#     attention_masks = encoding[\"attention_mask\"].to(device)\n","\n","#     # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n","#     beam_outputs = model.generate(\n","#         input_ids=input_ids,\n","#         attention_mask=attention_masks,\n","#         do_sample=True,\n","#         max_length=256,\n","#         num_beams=2,\n","#         top_k=70,\n","#         top_p=0.95,\n","#         early_stopping=True,\n","#         num_return_sequences=1,\n","#     )\n","#     sent = tokenizer.decode(beam_outputs[0].tolist(), skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","#     return sent\n","\n","\n","def generate(sentence, preprocessor=None):\n","    '''\n","    This function is for Joint model to generate/predict\n","    '''\n","\n","    # For T5\n","    #sentence = 'summarize: ' + sentence\n","\n","    encoding = summarizer_tokenizer(\n","        [sentence],\n","        max_length = 512,\n","        truncation = True,\n","        padding = 'max_length',\n","        return_tensors = 'pt',\n","    )\n","\n","    summary_ids = summarizer.generate(\n","        encoding['input_ids'].to(device),\n","        num_beams = 15,\n","        min_length = 30,\n","        max_length = 512,\n","        top_k = 80, top_p = 0.97\n","    ).to(device)\n","\n","    # For T5\n","    # for i, summary_id in enumerate(summary_ids):\n","    #     add_tokens = torch.tensor([18356, 10]).to(device)\n","    #     summary_ids[i,:] = torch.cat((summary_id, add_tokens), dim=0)[:-2]\n","\n","    summary_atten_mask = torch.ones(summary_ids.shape).to(device)\n","    summary_atten_mask[summary_ids[:,:] == summarizer_tokenizer.pad_token_id] = 0\n","\n","    beam_outputs = simplifier.generate(\n","        input_ids = summary_ids,\n","        attention_mask = summary_atten_mask,\n","        do_sample = True,\n","        max_length = 256,\n","        num_beams = 5, #16\n","        top_k = 80,  #120\n","        top_p = 0.95, #0.95\n","        early_stopping = True,\n","        num_return_sequences = 1,\n","    )\n","\n","    sent = simplifier_tokenizer.decode(beam_outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    return sent\n","\n","    '''\n","    sentence = preprocessor.encode_sentence(sentence)\n","    text = \"simplify: \" + sentence\n","    encoding = tokenizer(text, max_length=max_len,\n","                                     padding='max_length',\n","                                     truncation=True,\n","                                     return_tensors=\"pt\")\n","    input_ids = encoding[\"input_ids\"].to(device)\n","    attention_masks = encoding[\"attention_mask\"].to(device)\n","\n","    # set top_k = 130 and set top_p = 0.97 and num_return_sequences = 1\n","    beam_outputs = model.generate(\n","        input_ids=input_ids,\n","        attention_mask=attention_masks,\n","        do_sample=False,\n","        max_length=max_len,\n","        num_beams=10,\n","        top_k=130,\n","        top_p=0.97,\n","        early_stopping=True,\n","        num_return_sequences=1\n","    )\n","    pred_sent = tokenizer.decode(beam_outputs[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    return pred_sent\n","    # final_outputs = []\n","    # for beam_output in beam_outputs:\n","    #     sent = tokenizer.decode(beam_output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    #     if sent.lower() != sentence.lower() and sent not in final_outputs:\n","    #         final_outputs.append(sent)\n","\n","    # return final_outputs\n","    '''\n","\n","\n","def evaluate(orig_filepath, sys_filepath, ref_filepaths):\n","    orig_sents = read_lines(orig_filepath)\n","    # NOTE: change the refs_sents if several references are used\n","    refs_sents = [read_lines(ref_filepaths)]\n","    #refs_sents = [read_lines(filepath) for filepath in ref_filepaths]\n","\n","    return corpus_sari(orig_sents, read_lines(sys_filepath), refs_sents)\n","\n","# def evaluate_on(dataset, features_kwargs, phase, model_dirname=None, checkpoint_path=None):\n","\n","#     global model, tokenizer, device, model_dir, _model_dirname, max_len\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     model_path = EXP_DIR / model_dirname / checkpoint_path\n","#     Model = T5FineTuner.load_from_checkpoint(model_path).to(device)\n","#     model = Model.model\n","#     tokenizer = Model.tokenizer\n","#     model_dir = EXP_DIR / model_dirname\n","\n","#     # load_model(model_dirname)\n","#     preprocessor = Preprocessor(features_kwargs)\n","#     output_dir = model_dir / \"outputs\"\n","#     # output_dir = REPO_DIR / f\"outputs/{_model_dirname}\"\n","#     output_dir.mkdir(parents=True, exist_ok=True)\n","#     print(\"Output dir: \", output_dir)\n","#     features_hash = generate_hash(features_kwargs)\n","#     output_score_filepath = output_dir / f\"score_{features_hash}_{dataset}_{phase}_log.txt\"\n","#     if not output_score_filepath.exists() or count_line(output_score_filepath) == 0:\n","#         # log_params(output_dir / f\"{features_hash}_features_kwargs.json\", features_kwargs)\n","#         start_time = time.time()\n","#         complex_filepath = get_data_filepath(dataset, phase, 'complex')\n","#         pred_filepath = output_dir / f'{features_hash}_{complex_filepath.stem}.txt'\n","#         # ref_filepaths = [get_data_filepath(dataset, phase, 'simple.turk', i) for i in range(8)]\n","#         ref_filepath = get_data_filepath(dataset, phase, 'simple')\n","#         print(pred_filepath)\n","#         if pred_filepath.exists() and count_line(pred_filepath) == count_line(complex_filepath):\n","#             print(\"File is already processed.\")\n","#         else:\n","#             simplify_file(complex_filepath, pred_filepath, preprocessor)\n","\n","#         # print(\"Evaluate: \", pred_filepath)\n","#         with log_stdout(output_score_filepath):\n","#             # print(\"features_kwargs: \", features_kwargs)\n","\n","#             # refs = [[line] for line in yield_lines(ref_filepath)]\n","#             # score = corpus_sari(read_lines(complex_filepath), read_lines(pred_filepath), refs)\n","#             # print(len(read_lines(complex_filepath)), len(read_lines(pred_filepath)) )\n","#             # print([len(s) for s in refs])\n","\n","#             # scores = get_all_scores(read_lines(complex_filepath), read_lines(pred_filepath), refs)\n","#             score = evaluate(complex_filepath, pred_filepath, [ref_filepath])\n","#             # scores = evaluate_all_metrics(complex_filepath, pred_filepath, ref_filepaths)\n","#             if \"WordRatioFeature\" in features_kwargs:\n","#                 print(\"W:\", features_kwargs[\"WordRatioFeature\"][\"target_ratio\"], \"\\t\", end=\"\")\n","#             if \"CharRatioFeature\" in features_kwargs:\n","#                 print(\"C:\", features_kwargs[\"CharRatioFeature\"][\"target_ratio\"], \"\\t\", end=\"\")\n","#             if \"LevenshteinRatioFeature\" in features_kwargs:\n","#                 print(\"L:\", features_kwargs[\"LevenshteinRatioFeature\"][\"target_ratio\"], \"\\t\", end=\"\")\n","#             if \"WordRankRatioFeature\" in features_kwargs:\n","#                 print(\"WR:\", features_kwargs[\"WordRankRatioFeature\"][\"target_ratio\"], \"\\t\", end=\"\")\n","#             if \"DependencyTreeDepthRatioFeature\" in features_kwargs:\n","#                 print(\"DTD:\", features_kwargs[\"DependencyTreeDepthRatioFeature\"][\"target_ratio\"], \"\\t\", end=\"\")\n","#             print(\"{:.2f} \\t \".format(score))\n","\n","#             print(\"Execution time: --- %s seconds ---\" % (time.time() - start_time))\n","#     else:\n","#         print(\"Already exist: \", output_score_filepath)\n","#         print(\"\".join(read_lines(output_score_filepath)))\n","\n","def back_translation(text):\n","    X = translator.translate(text, dest = 'de')\n","    return translator.translate(X.text, dest = 'en').text\n","\n","\n","def simplify_file(complex_filepath, output_filepath, features_kwargs=None, model_dirname=None, post_processing=True):\n","    '''\n","    Obtain the simplified sentences (predictions) from the original complex sentences.\n","    '''\n","\n","    total_lines = count_line(complex_filepath)\n","    print(complex_filepath)\n","    print(complex_filepath.stem)\n","\n","    output_file = Path(output_filepath).open(\"w\")\n","\n","    for n_line, complex_sent in enumerate(yield_lines(complex_filepath), start=1):\n","        ### NOTE: Change it when using Single model or Joint model\n","        #output_sents = generate_single(complex_sent, preprocessor = None)\n","        output_sents = generate(complex_sent, preprocessor=None)\n","\n","\n","        print(f\"{n_line}/{total_lines}\", \" : \", output_sents)\n","        if output_sents:\n","            # output_file.write(output_sents[0] + \"\\n\")\n","            output_file.write(output_sents + \"\\n\")\n","        else:\n","            output_file.write(\"\\n\")\n","    output_file.close()\n","\n","    if post_processing: post_process(output_filepath)\n","\n","def post_process(filepath):\n","    lines = []\n","    for line in yield_lines(filepath):\n","        lines.append(line.replace(\"''\", '\"'))\n","    write_lines(lines, filepath)\n","\n","def evaluate_on_MILDSUM(phase, features_kwargs=None,  model_dirname = None):\n","    dataset = MILDSUM\n","    #model_dir = EXP_DIR / model_dirname\n","    output_dir = '/content/outputs'\n","\n","    #output_dir.mkdir(parents = True, exist_ok = True)\n","\n","    if not os.path.exists(output_dir):\n","      os.makedirs(output_dir)\n","\n","    #features_hash = generate_hash(features_kwargs)\n","    output_score_filepath = f'{output_dir}/score_{dataset}_{phase}.log.txt'\n","    #complex_filepath = get_data_filepath(dataset, phase, 'complex_kw_num4_div0.9') # _kw_num3_div0.9'\n","    complex_filepath = get_data_filepath(dataset, phase, 'complex')\n","\n","    if not os.path.exists(output_score_filepath) or count_line(output_score_filepath)==0:\n","        start_time = time.time()\n","        #complex_filepath =get_data_filepath(dataset, phase, 'complex_kw_num4_div0.9')\n","        complex_filepath =get_data_filepath(dataset, phase, 'complex')\n","        complex_filepath = Path(complex_filepath)\n","\n","        #complex_filepath = get_data_filepath(dataset, phase, 'complex_summary_'+str(ratio))\n","        pred_filepath = f'{output_dir}/{complex_filepath.stem}.txt'\n","        ref_filepaths = get_data_filepath(dataset, phase, 'simple')\n","\n","        if os.path.exists(pred_filepath) and count_line(pred_filepath)==count_line(complex_filepath):\n","            print(\"File is already processed.\")\n","        else:\n","            simplify_file(complex_filepath, pred_filepath, features_kwargs, model_dirname)\n","\n","        print(\"Evaluate: \", pred_filepath)\n","\n","        with log_stdout(output_score_filepath):\n","\n","            scores  = evaluate_system_output(test_set='custom',\n","                                             sys_sents_path=str(pred_filepath),\n","                                             orig_sents_path=str(complex_filepath),\n","                                             refs_sents_paths=str(ref_filepaths),metrics = ['bleu', 'sari', 'fkgl'] )\n","\n","            sys_sents = get_sys_sents(test_set = 'custom', sys_sents_path=str(pred_filepath))\n","            orig_sents, refs_sents = get_orig_and_refs_sents(test_set = 'custom',\n","                                                             orig_sents_path = str(complex_filepath),\n","                                                             refs_sents_paths = str(ref_filepaths))\n","\n","\n","            print(\"SARI: {:.2f}\\t D-SARI: {:.2f} \\t BLEU: {:.2f} \\t FKGL: {:.2f} \".format(scores['sari'],\n","                                                                                          D_SARI_file(orig_sents,sys_sents,refs_sents[0],),\n","                                                                                          scores['bleu'],\n","                                                                                          scores['fkgl']))\n","            # print(\"{:.2f} \\t {:.2f} \\t {:.2f} \".format(scores['SARI'], scores['BLEU'], scores['FKGL']))\n","\n","            print(\"Execution time: --- %s seconds ---\" % (time.time() - start_time))\n","            return None\n","    else:\n","        print(\"Already exists: \", output_score_filepath)\n","        print(\"\".join(read_lines(output_score_filepath)))\n","\n","evaluate_on_MILDSUM(phase='test', features_kwargs=None, model_dirname=None)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
